# Logstash Pipeline for IPC Validator Logs

input {
  # Filebeat input from validators
  beats {
    port => 5044
    type => "ipc-logs"
  }
}

filter {
  # Parse systemd journal fields
  if [systemd] {
    mutate {
      add_field => { "log_source" => "systemd" }
    }
  }

  # Parse file-based logs
  if [log][file][path] {
    mutate {
      add_field => { "log_source" => "file" }
    }
  }

  # Detect log level from message
  grok {
    match => {
      "message" => [
        "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:log_level}\s+%{GREEDYDATA:log_message}",
        "\[%{TIMESTAMP_ISO8601:timestamp}\]\s+%{LOGLEVEL:log_level}\s+%{GREEDYDATA:log_message}",
        "%{LOGLEVEL:log_level}:\s+%{GREEDYDATA:log_message}",
        "%{GREEDYDATA:log_message}"
      ]
    }
    overwrite => ["message"]
    tag_on_failure => ["_grok_parse_failure"]
  }

  # Parse CometBFT consensus messages
  if [container][name] == "ipc-node" or [systemd][unit] == "ipc-node.service" {
    grok {
      match => {
        "message" => [
          # Block committed
          "Committed state.*height=%{NUMBER:block_height:int}.*txs=%{NUMBER:tx_count:int}",
          # New block
          "Finalizing commit of block.*height=%{NUMBER:block_height:int}",
          # Consensus round
          "enterNewRound.*height=%{NUMBER:block_height:int}.*round=%{NUMBER:consensus_round:int}",
          # Proposal
          "Received proposal.*height=%{NUMBER:block_height:int}",
          # Vote
          "Signed and pushed vote.*height=%{NUMBER:block_height:int}"
        ]
      }
      add_tag => ["cometbft_consensus"]
      tag_on_failure => []
    }
  }

  # Parse checkpoint relayer messages
  if [container][name] == "ipc-relayer" or [systemd][unit] == "ipc-relayer.service" {
    grok {
      match => {
        "message" => [
          # Checkpoint submission
          "submitting checkpoint.*height=%{NUMBER:checkpoint_height:int}",
          "checkpoint submitted.*hash=%{DATA:checkpoint_hash}",
          # Error patterns
          "checkpoint submission failed.*%{GREEDYDATA:error_detail}"
        ]
      }
      add_tag => ["checkpoint_relayer"]
      tag_on_failure => []
    }
  }

  # Parse Ethereum/FEVM transactions
  if "eth" in [message] or "transaction" in [message] {
    grok {
      match => {
        "message" => [
          "tx hash.*0x%{DATA:tx_hash}",
          "from.*0x%{DATA:from_address}",
          "gas.*%{NUMBER:gas_used:int}"
        ]
      }
      add_tag => ["ethereum_tx"]
      tag_on_failure => []
    }
  }

  # Extract error details
  if [log_level] =~ /(?i)(error|err|fatal|panic)/ {
    mutate {
      add_tag => ["error"]
    }
  }

  # Extract warning details
  if [log_level] =~ /(?i)(warn|warning)/ {
    mutate {
      add_tag => ["warning"]
    }
  }

  # Normalize log level
  if [log_level] {
    mutate {
      uppercase => ["log_level"]
    }
  }

  # Parse timestamp if available
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ", "yyyy-MM-dd HH:mm:ss"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
  }

  # Add additional metadata
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "ipc-logs"
    }
  }

  # Cleanup
  mutate {
    remove_field => ["agent", "ecs", "input", "host.name"]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
    index => "ipc-logs-%{[agent][hostname]}-%{+YYYY.MM.dd}"

    # Use data stream for better management (Elasticsearch 7.9+)
    # data_stream => "true"
    # data_stream_type => "logs"
    # data_stream_dataset => "ipc.validator"
    # data_stream_namespace => "production"
  }

  # Debug output (comment out in production)
  # stdout {
  #   codec => rubydebug
  # }
}

